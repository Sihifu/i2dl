{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Optional: Batch Normalization & Dropout \n",
    "\n",
    "In this Notebook we will introduce the idea of Batch Normalization and Dropout and how both these methods help in Neural Network training. \n",
    "\n",
    "By completing this exercise you will:\n",
    "1. Know the implementation details of Batch Norm and Dropout\n",
    "2. Notice the difference in behaviour during train and test time\n",
    "3. Use Batch Norm and Dropout in a Fully Connected Layer to see how it affects training\n",
    "\n",
    "Let us start with Batch Normalization:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Batch Normalization\n",
    "\n",
    "## 1.1 What is Batch Normalization\n",
    "One way to make deep networks easier to train is to use more sophisticated optimization procedures such as SGD+momentum, RMSProp, or Adam. Another strategy is to change the architecture of the network to make it easier to train. One idea along these lines is batch normalization, which was proposed by [1].\n",
    "\n",
    "The idea is relatively straightforward. Machine learning methods tend to work better when their input data consists of uncorrelated features with zero mean and unit variance. When training a neural network, we can preprocess the data before feeding it to the network to explicitly decorrelate its features; this will ensure that the first layer of the network sees data that follows a nice distribution. However, even if we preprocess the input data, the activations at deeper layers of the network will likely no longer be decorrelated and will no longer have zero mean or unit variance since they are output from earlier layers in the network. Even worse, during the training process, the distribution of features at each layer of the network will shift as the weights of each layer are updated.\n",
    "\n",
    "The authors of [1] hypothesize that the shifting distribution of features inside deep neural networks may make training deep networks more difficult. To overcome this problem, [1] proposes to insert batch normalization layers into the network. At training time, a batch normalization layer uses a minibatch of data to estimate the mean and standard deviation of each feature. These estimated means and standard deviations are then used to center and normalize the features of the minibatch. A running average of these means and standard deviations is kept during training, and at test time, these running averages are used to center and normalize features.\n",
    "\n",
    "It is possible that this normalization strategy could reduce the representational power of the network since it may sometimes be optimal for certain layers to have features that are not zero-mean or unit variance. To this end, the batch normalization layer includes learnable shift and scale parameters for each feature dimension.\n",
    "\n",
    "[1] Sergey Ioffe and Christian Szegedy, \"Batch Normalization: Accelerating Deep Network Training by Reducing\n",
    "Internal Covariate Shift\", ICML 2015.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Before We Start\n",
    "\n",
    "It is important that we take a look at the Mathematical formula behind Batch Norm. Please make sure to understand the formula below since this will definitely help you with the implementation later. :)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<!-- In case the image does not show, uncomment the following:\n",
    "<img name=\"batchnorm\" src=\"./img/batchnorm.jpg\">\n",
    " -->\n",
    "<img name=\"batchnorm\" src=\"https://i2dl.vc.in.tum.de/static/images/exercise_08/batchnorm.jpg\">\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A quick explanation of the formula\n",
    "It may look a bit confusing at first glance, but that's not true. Let's summarize the mathematics here: In the left column, we are given the input data, which consists (as always) of $N$ samples of dimension $D$. Furthermore, we need the learnable shift and scale parameters which we call $\\beta$ and $\\gamma$. The intermediates describe the mean $\\mu$ and variance $\\sigma$ that we need to compute from the input data and then $\\hat{x}$ which is the normalized input data. The output is given by $y$ which is the combination of the normalized data with the learnable parameters. \n",
    "\n",
    "The right column contains the mathematical formulations and can be summarized as follows:\n",
    "1. For the given input x, we calculate the mean $\\mu$ across all input samples.\n",
    "2. Based on the mean $\\mu$, we compute the variances $\\sigma$ of each value in the sample.\n",
    "3. We then normalize the input data based on the computed mean and variance.\n",
    "4. Finally, we combine the normalized data with the learnable parameters $\\gamma$ and $\\beta$.\n",
    "\n",
    "Please remember that Batch Normalization behaves differently at training and test time. In the figure above, we see the behavior at training time. \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (Optional) Mount folder in Colab\n",
    "\n",
    "Uncomment thefollowing cell to mount your gdrive if you are using the notebook in google colab:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\nfrom google.colab import drive\\nimport os\\ngdrive_path='/content/gdrive/MyDrive/i2dl/exercise_08'\\n\\n# This will mount your google drive under 'MyDrive'\\ndrive.mount('/content/gdrive', force_remount=True)\\n# In order to access the files in this notebook we have to navigate to the correct folder\\nos.chdir(gdrive_path)\\n# Check manually if all files are present\\n\""
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Use the following lines if you want to use Google Colab\n",
    "# Don't forget to change to GPU. Runtime --> Change runtime type --> GPU\n",
    "# NOTE: terminate all other colab sessions that use GPU!\n",
    "# NOTE 2: Make sure the correct exercise folder (e.g exercise_08) is given.\n",
    "# NOTE 3: For simplicity, create a folder \"i2dl\" within your main drive folder, and put the exercise there.\n",
    "\n",
    "\"\"\"\n",
    "from google.colab import drive\n",
    "import os\n",
    "gdrive_path='/content/gdrive/MyDrive/i2dl/exercise_08'\n",
    "\n",
    "# This will mount your google drive under 'MyDrive'\n",
    "drive.mount('/content/gdrive', force_remount=True)\n",
    "# In order to access the files in this notebook we have to navigate to the correct folder\n",
    "os.chdir(gdrive_path)\n",
    "# Check manually if all files are present\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2 Batch Normalization: Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "# As usual, a bit of setup\n",
    "\n",
    "import time\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from exercise_code.layers import *\n",
    "from exercise_code.tests import *\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import os\n",
    "import shutil\n",
    "\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "from exercise_code.BatchNormModel import SimpleNetwork, BatchNormNetwork, DropoutNetwork\n",
    "\n",
    "%matplotlib inline\n",
    "plt.rcParams['figure.figsize'] = (10.0, 8.0) # set default size of plots\n",
    "plt.rcParams['image.interpolation'] = 'nearest'\n",
    "plt.rcParams['image.cmap'] = 'gray'\n",
    "\n",
    "# for auto-reloading external modules\n",
    "# see http://stackoverflow.com/questions/1907993/autoreload-of-modules-in-ipython\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "os.environ['KMP_DUPLICATE_LIB_OK']='True' # To prevent the kernel from dying.\n",
    "\n",
    "# supress cluttering warnings in solutions\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "def rel_error(x, y):\n",
    "    \"\"\" returns relative error \"\"\"\n",
    "    return np.max(np.abs(x - y) / (np.maximum(1e-8, np.abs(x) + np.abs(y))))\n",
    "\n",
    "\n",
    "device = torch.device(\"mps\" if torch.backends.mps.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Batch normalization: Forward Pass\n",
    "\n",
    "<div class=\"alert alert-success\">\n",
    "    <h3>Task: Check Code</h3>\n",
    "    <p>In the file <code>exercise_code/layers.py </code>, we have implemented the <code>batchnorm_forward</code> function. Read this implementation and make sure you understand what batch normalization is doing. Then execute the following cells to test the implementation.\n",
    " </p>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before batch normalization:\n",
      "  means:  [66.7252611   3.58156532 22.65774279]\n",
      "  stds:  [34.53939906 29.5334618  36.03938517] \n",
      "\n",
      "After batch normalization with (gamma=1, beta=0)\n",
      "  mean:  [ 1.04249942e-15 -8.49320614e-17 -2.06779038e-16]\n",
      "  std:  [1.         0.99999999 1.        ] \n",
      "\n",
      "After batch normalization with (nontrivial gamma, beta)\n",
      "  means:  [11. 12. 13.]\n",
      "  stds:  [1.         1.99999999 2.99999999]\n"
     ]
    }
   ],
   "source": [
    "# Check the training-time forward pass by checking means and variances\n",
    "# of features both before and after batch normalization\n",
    "\n",
    "# Simulate the forward pass for a two-layer network\n",
    "N, D1, D2, D3 = 200, 50, 60, 3\n",
    "X = np.random.randn(N, D1)\n",
    "W1 = np.random.randn(D1, D2)\n",
    "W2 = np.random.randn(D2, D3)\n",
    "a = np.maximum(0, X.dot(W1)).dot(W2)\n",
    "\n",
    "print('Before batch normalization:')\n",
    "print('  means: ', a.mean(axis=0))\n",
    "print('  stds: ', a.std(axis=0), '\\n')\n",
    "\n",
    "# Means should be close to zero and stds close to one\n",
    "print('After batch normalization with (gamma=1, beta=0)')\n",
    "a_norm, _ = batchnorm_forward(a, np.ones(D3), np.zeros(D3), {'mode': 'train'})\n",
    "print('  mean: ', a_norm.mean(axis=0))\n",
    "print('  std: ', a_norm.std(axis=0) , '\\n')\n",
    "\n",
    "# Now means should be close to beta and stds close to gamma\n",
    "beta = np.asarray([11.0, 12.0, 13.0])\n",
    "gamma = np.asarray([1.0, 2.0, 3.0])\n",
    "a_norm, _ = batchnorm_forward(a, gamma, beta, {'mode': 'train'})\n",
    "print('After batch normalization with (nontrivial gamma, beta)')\n",
    "print('  means: ', a_norm.mean(axis=0))\n",
    "print('  stds: ', a_norm.std(axis=0) )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since the mean and variances in batch norm are computed in training time,\n",
    "before invoking the test-time forward pass run the training-time\n",
    "forward pass (previous cell) many times to warm up the running averages. Then\n",
    "checking the means and variances of activations for a test-time\n",
    "forward pass."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After batch normalization (test-time):\n",
      "  means:  [-0.06739308 -0.03260452  0.12149349]\n",
      "  stds:  [1.03655268 1.08899534 0.9860315 ]\n"
     ]
    }
   ],
   "source": [
    "# Check the test-time forward pass by checking means and variances \n",
    "# of features after batch normalization\n",
    "\n",
    "N, D1, D2, D3 = 200, 50, 60, 3\n",
    "W1 = np.random.randn(D1, D2)\n",
    "W2 = np.random.randn(D2, D3)\n",
    "\n",
    "bn_param = {'mode': 'train'}\n",
    "gamma = np.ones(D3)\n",
    "beta = np.zeros(D3)\n",
    "for t in range(50):\n",
    "    X = np.random.randn(N, D1)\n",
    "    a = np.maximum(0, X.dot(W1)).dot(W2)\n",
    "    batchnorm_forward(a, gamma, beta, bn_param)\n",
    "bn_param['mode'] = 'test'\n",
    "X = np.random.randn(N, D1)\n",
    "a = np.maximum(0, X.dot(W1)).dot(W2)\n",
    "a_norm, _ = batchnorm_forward(a, gamma, beta, bn_param)\n",
    "\n",
    "# Means should be close to zero and stds close to one, but will be\n",
    "# noisier than training-time forward passes.\n",
    "print('After batch normalization (test-time):')\n",
    "print('  means: ', a_norm.mean(axis=0))\n",
    "print('  stds: ', a_norm.std(axis=0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Batch Normalization: Backward Pass\n",
    "Since batch normalization is realized by a more complex function of learnable parameters, it is a good exercise to train your backprop skills through this computational graph.\n",
    "\n",
    "<div class=\"alert alert-info\">\n",
    "    <h3>Task: Implement</h3>\n",
    "    <p>Open <code>exercise_code/layers.py</code> and implement the backward pass for Batch Normalization in the function <code> batchnorm_backward() </code>.\n",
    "    </p>\n",
    "    <p> To derive the backward pass you should write out the computation graph for batch normalization and backprop through each of the intermediate nodes. Some intermediates may have multiple outgoing branches; make sure to sum gradients across these branches in the backward pass. You can stay close to the forward pass implementation we have provided for you, i.e. go line by line backward.\n",
    "    </p>\n",
    "    <p> Once you have finished, run the following to numerically check your backward pass.\n",
    "    </p>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dx error:  1.0\n",
      "dgamma error:  1.1346832511185275e-11\n",
      "dbeta error:  3.2756070377210013e-12\n"
     ]
    }
   ],
   "source": [
    "# Gradient check batchnorm backward pass\n",
    "\n",
    "N, D = 4, 5\n",
    "x = 5 * np.random.randn(N, D) + 12\n",
    "gamma = np.random.randn(D)\n",
    "beta = np.random.randn(D)\n",
    "dout = np.random.randn(N, D)\n",
    "\n",
    "bn_param = {'mode': 'train'}\n",
    "\n",
    "fx = lambda x: batchnorm_forward(x, gamma, beta, bn_param)[0]\n",
    "fg = lambda a: batchnorm_forward(x, gamma, beta, bn_param)[0]\n",
    "fb = lambda b: batchnorm_forward(x, gamma, beta, bn_param)[0]\n",
    "\n",
    "dx_num = eval_numerical_gradient_array(fx, x, dout)\n",
    "da_num = eval_numerical_gradient_array(fg, gamma, dout)\n",
    "db_num = eval_numerical_gradient_array(fb, beta, dout)\n",
    "\n",
    "_, cache = batchnorm_forward(x, gamma, beta, bn_param)\n",
    "dx, dgamma, dbeta = batchnorm_backward(dout, cache)\n",
    "\n",
    "print('dx error: ', rel_error(dx_num, dx))\n",
    "print('dgamma error: ', rel_error(da_num, dgamma))\n",
    "print('dbeta error: ', rel_error(db_num, dbeta))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-1.56889037,  0.41527241,  0.38146467,  1.05067832,  0.74929595],\n",
       "       [ 1.07306271, -0.44051845, -0.18429953, -0.48061534,  1.39004669],\n",
       "       [ 0.85602603, -0.34622404,  0.03752929, -1.12034609, -0.93925231],\n",
       "       [-0.36019837,  0.37147009, -0.23469443,  0.55028311, -1.20009033]])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f_test = lambda x: x-np.mean(x,axis=0,keepdims=True)\n",
    "d_test=eval_numerical_gradient_array(f_test, x, dout)\n",
    "d_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Upstream gradient dout:\n",
      " tensor([[-1.0875, -1.2418, -1.3451,  0.2568, -0.9724],\n",
      "        [ 0.2225,  1.4266,  0.6998, -0.0856,  0.5807],\n",
      "        [ 0.8803,  0.9766, -0.2856, -2.0527, -0.4034],\n",
      "        [ 0.9680,  0.7888, -1.7069, -0.0729, -0.2505]], dtype=torch.float64)\n",
      "Computed gradient from PyTorch dx:\n",
      " tensor([[-1.3333, -1.7293, -0.6857,  0.7454, -0.7110],\n",
      "        [-0.0233,  0.9390,  1.3593,  0.4030,  0.8421],\n",
      "        [ 0.6345,  0.4891,  0.3739, -1.5641, -0.1420],\n",
      "        [ 0.7222,  0.3013, -1.0474,  0.4157,  0.0109]])\n"
     ]
    }
   ],
   "source": [
    "# Custom Module for f(x) = x - mean(x, axis=0)\n",
    "class SubtractBatchMean(nn.Module):\n",
    "    def forward(self, x):\n",
    "        # Subtract batch mean along axis 0 (mean over batch dimension)\n",
    "        return x - torch.mean(x, dim=0, keepdim=True)\n",
    "\n",
    "x = torch.randn(N, D, requires_grad=True)  # Input tensor with gradients\n",
    "# Instantiate the module\n",
    "sub_mean = SubtractBatchMean()\n",
    "\n",
    "# Forward pass\n",
    "out = sub_mean(x)\n",
    "dout=torch.tensor(dout)\n",
    "# Backward pass: compute gradients with respect to x\n",
    "out.backward(dout)\n",
    "\n",
    "# Get the gradient with respect to input x\n",
    "grad_x = x.grad\n",
    "\n",
    "# Print results\n",
    "print(\"Upstream gradient dout:\\n\", dout)\n",
    "print(\"Computed gradient from PyTorch dx:\\n\", grad_x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-1.3333, -1.7293, -0.6857,  0.7454, -0.7110],\n",
       "        [-0.0233,  0.9390,  1.3593,  0.4030,  0.8421],\n",
       "        [ 0.6345,  0.4891,  0.3739, -1.5641, -0.1420],\n",
       "        [ 0.7222,  0.3013, -1.0474,  0.4157,  0.0109]], dtype=torch.float64)"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "N=dout.shape[0]\n",
    "d_x_minus_mu=dout-torch.mean(dout,axis=0,keepdims=True)\n",
    "d_x_minus_mu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Upstream gradient dout:\n",
      " tensor([[-1.0875, -1.2418, -1.3451,  0.2568, -0.9724],\n",
      "        [ 0.2225,  1.4266,  0.6998, -0.0856,  0.5807],\n",
      "        [ 0.8803,  0.9766, -0.2856, -2.0527, -0.4034],\n",
      "        [ 0.9680,  0.7888, -1.7069, -0.0729, -0.2505]], dtype=torch.float64)\n",
      "Computed gradient from PyTorch dx:\n",
      " tensor([[-0.0162, -0.7202, -0.2116,  0.0914, -0.4682],\n",
      "        [-0.3455,  0.3195,  1.5167,  0.3031,  0.7408],\n",
      "        [ 0.0824,  0.7699,  0.8364, -1.6077, -0.2318],\n",
      "        [ 0.2793, -0.3692, -2.1415,  1.2132, -0.0408]])\n"
     ]
    }
   ],
   "source": [
    "# Custom Module for f(x) = x - mean(x, axis=0)\n",
    "class BatchNorm(nn.Module):\n",
    "    def forward(self, x):\n",
    "        # Subtract batch mean along axis 0 (mean over batch dimension)\n",
    "        y= x - torch.mean(x, dim=0, keepdim=True)\n",
    "        var = torch.var(x, dim=0, keepdim=True)\n",
    "        eps=1e-5\n",
    "        return y/torch.sqrt(var+eps)\n",
    "\n",
    "\n",
    "x = torch.randn(N, D, requires_grad=True)  # Input tensor with gradients\n",
    "# Instantiate the module\n",
    "norm = BatchNorm()\n",
    "\n",
    "# Forward pass\n",
    "out = norm(x)\n",
    "dout=torch.tensor(dout)\n",
    "# Backward pass: compute gradients with respect to x\n",
    "out.backward(dout)\n",
    "\n",
    "# Get the gradient with respect to input x\n",
    "grad_x = x.grad\n",
    "\n",
    "# Print results\n",
    "print(\"Upstream gradient dout:\\n\", dout)\n",
    "print(\"Computed gradient from PyTorch dx:\\n\", grad_x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "f_test= lambda x: x-torch.mean(x,axis=0,keepdim=True)\n",
    "f_sum= lambda x: torch.sum(x,axis=0,keepdim=True)\n",
    "f_norm= lambda x: f_test(x)/torch.sqrt(f_sum((f_test(x))**2)+1e-5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-1.4942, -0.9927,  1.2566, -1.2607, -1.1921],\n",
       "        [ 0.5197,  0.6639, -0.9366, -0.3157, -0.3841],\n",
       "        [ 0.5929, -0.7073,  0.3361,  0.6032,  1.0956],\n",
       "        [ 0.3816,  1.0361, -0.6562,  0.9732,  0.4806]], grad_fn=<DivBackward0>)"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.8627, -0.5732,  0.7255, -0.7279, -0.6883],\n",
       "        [ 0.3001,  0.3833, -0.5407, -0.1823, -0.2218],\n",
       "        [ 0.3423, -0.4083,  0.1941,  0.3483,  0.6326],\n",
       "        [ 0.2203,  0.5982, -0.3789,  0.5619,  0.2775]], grad_fn=<DivBackward0>)"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f_norm(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-11.2330, -11.9721,  -0.0676,  -0.2226,  -1.1867],\n",
      "        [ -0.1967,   6.5007,   0.1341,  -0.1203,   1.4055],\n",
      "        [  5.3453,   3.3857,   0.0369,   0.4671,  -0.2370],\n",
      "        [  6.0844,   2.0856,  -0.1033,  -0.1241,   0.0182]],\n",
      "       dtype=torch.float64, grad_fn=<DivBackward0>)\n"
     ]
    }
   ],
   "source": [
    "d_x_minus_mu=dout-torch.mean(dout,axis=0,keepdim=True)\n",
    "denom=1/(torch.var(x, dim=0, keepdim=True)+1e-5)**2\n",
    "term_1=d_x_minus_mu*(torch.var(x, dim=0, keepdim=True)+1e-5)\n",
    "term_2=torch.mean(x,axis=0,keepdim=True)*(1/(2*torch.sqrt(torch.var(x, dim=0, keepdim=True)+1e-5)))*2*d_x_minus_mu\n",
    "\n",
    "print((term_1-term_2)/denom)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.3 Using Batch Normalization with PyTorch\n",
    "\n",
    "Now that we have seen the implementation of Batch Normalization, it is interesting to see how it would affect the overall Model Performance. Since you have already worked with PyTorch in the last exercise, you have seen how easy it makes our lives. As an experiment, we will use a simple Fully Connected Network in PyTorch here."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setup TensorBoard\n",
    "In exercise 07 you've already learned how to use TensorBoard. Let's use it again to make the debugging of our network and training process more convenient! Throughout this notebook, feel free to add further logs or visualizations your TensorBoard!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Few Hyperparameters before we start things off\n",
    "hidden_dim = 200\n",
    "batch_size = 50\n",
    "\n",
    "logdir = './logs'\n",
    "if os.path.exists(logdir):\n",
    "    # We delete the logs on the first run\n",
    "    shutil.rmtree(logdir)\n",
    "os.mkdir(logdir)\n",
    "\n",
    "epochs = 5\n",
    "learning_rate = 0.00005"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "################# COLAB ONLY #################\n",
    "# %load_ext tensorboard\n",
    "# %tensorboard --logdir=./ --port 6006\n",
    "\n",
    "# Use the cmd for less trouble, if you can. From the working directory, run: tensorboard --logdir=./ --port 6006"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train a model without Batch Normalization. \n",
    "\n",
    "<div class=\"alert alert-success\">\n",
    "    <h3>Task: Check Code</h3>\n",
    "    <p>Let us first start with a simple network which does not make use of Batch Normalization. We have already implemented the a simple network <code>SimpleNetwork</code> in <code>exercise_code/BatchNormModel.py</code>. Feel free to check it out and play around with the parameters. The cell below is setting up a short trainings process for this network.\n",
    " </p>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "def create_tqdm_bar(iterable, desc):\n",
    "    return tqdm(enumerate(iterable),total=len(iterable), ncols=150, desc=desc)\n",
    "\n",
    "def train_model(model, train_loader, val_loader, loss_func, tb_logger, epochs=10, name='Autoencoder'):\n",
    "    \n",
    "    optimizer = model.configure_optimizer()\n",
    "    scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=epochs * len(train_loader) / 5, gamma=0.7)\n",
    "    validation_loss = 0\n",
    "    model = model.to(device)\n",
    "    for epoch in range(epochs):\n",
    "        \n",
    "        # Train\n",
    "        training_loop = create_tqdm_bar(train_loader, desc=f'Training Epoch [{epoch + 1}/{epochs}]')\n",
    "        training_loss = 0\n",
    "        for train_iteration, batch in training_loop:\n",
    "            optimizer.zero_grad()\n",
    "            loss = model.training_step(batch, loss_func)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            scheduler.step()\n",
    "            \n",
    "            training_loss += loss.item()\n",
    "\n",
    "            # Update the progress bar.\n",
    "            training_loop.set_postfix(train_loss = \"{:.8f}\".format(training_loss / (train_iteration + 1)), val_loss = \"{:.8f}\".format(validation_loss))\n",
    "\n",
    "            # Update the tensorboard logger.\n",
    "            tb_logger.add_scalar(f'{name}/train_loss', loss.item(), epoch * len(train_loader) + train_iteration)\n",
    "\n",
    "        # Validation\n",
    "        val_loop = create_tqdm_bar(val_loader, desc=f'Validation Epoch [{epoch + 1}/{epochs}]')\n",
    "        validation_loss = 0\n",
    "        with torch.no_grad():\n",
    "            for val_iteration, batch in val_loop:\n",
    "                loss = model.validation_step(batch, loss_func) # You need to implement this function.\n",
    "                validation_loss += loss.item()\n",
    "\n",
    "                # Update the progress bar.\n",
    "                val_loop.set_postfix(val_loss = \"{:.8f}\".format(validation_loss / (val_iteration + 1)))\n",
    "\n",
    "                # Update the tensorboard logger.\n",
    "                tb_logger.add_scalar(f'{name}/val_loss', validation_loss / (val_iteration + 1), epoch * len(val_loader) + val_iteration)\n",
    "        # This value is for the progress bar of the training loop.\n",
    "        validation_loss /= len(val_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train a simple model without batch normalization.\n",
    "\n",
    "model = SimpleNetwork(hidden_dim=hidden_dim, batch_size=batch_size, learning_rate=learning_rate).to(device)\n",
    "path = os.path.join('logs', 'bn_model_logs')\n",
    "if os.path.exists(path):\n",
    "    shutil.rmtree(path)\n",
    "path = os.path.join(path, f'simple-model')\n",
    "tb_logger = SummaryWriter(path)\n",
    "\n",
    "# Train the classifier.\n",
    "train_dl, val_dl, _ = model.prepare_data()\n",
    "\n",
    "epochs = 5\n",
    "loss_func = F.cross_entropy # The loss function we use for regression (Could also be nn.L1Loss()).\n",
    "train_model(model, train_dl, val_dl, loss_func, tb_logger, epochs=epochs, name='BatchNorm')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train a model incl. Batch Normalization\n",
    "\n",
    "<div class=\"alert alert-success\">\n",
    "    <h3>Task: Check Code</h3>\n",
    "    <p> Now that we have already seen how our simple network should work, let us look at a model that is actually using Batch Normalization. Again, we provide you with such a model <code>BatchNormNetwork</code> in <code>exercise_code/BatchNormModel.py</code>. Same as before: Feel free to check it out and play around with the parameters. The cell below is setting up a short trainings process for this model. \n",
    " </p>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = BatchNormNetwork(hidden_dim=hidden_dim, batch_size=batch_size, learning_rate=learning_rate)\n",
    "\n",
    "path = os.path.join('logs', 'bn_model_logs')\n",
    "path = os.path.join(path, f'BN-model')\n",
    "tb_logger = SummaryWriter(path)\n",
    "\n",
    "# Train the classifier.\n",
    "train_dl, val_dl, _ = model.prepare_data()\n",
    "\n",
    "epochs = 5\n",
    "loss_func = F.cross_entropy # The loss function we use for regression (Could also be nn.L1Loss()).\n",
    "train_model(model, train_dl, val_dl, loss_func, tb_logger, epochs=epochs, name='BatchNorm')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Observations\n",
    "Take a look at TensorBoard to compare the performance of both networks:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "################# COLAB ONLY #################\n",
    "# %load_ext tensorboard\n",
    "# %tensorboard --logdir=./ --port 6006\n",
    "\n",
    "# Use the cmd for less trouble, if you can. From the working directory, run: tensorboard --logdir=./ --port 6006"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see, using Batch Normalization resulted in better performance. You can easily observe lower validation loss and higher validation accuracy from the graphs. Batch Norm in general is helpful since it would lead to faster model training.\n",
    "\n",
    "Batch Normalization has other related benefits, for instance, it provides a bit of regularization. However, we would look for better methods of regularization such as Dropout. So in the second part of this notebook, let's have a more detailed look at the effect of Dropout. :)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Dropout"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1 What is Dropout\n",
    "\n",
    "Dropout [1] is a technique for regularizing neural networks by randomly setting some features to zero during the forward pass. While training, dropout is implemented by only keeping a neuron active with some probability p\n",
    "(a hyperparameter), or setting it to zero otherwise. The Dropout technique would help your Neural Network to perform better on Test data.\n",
    "\n",
    "We want to repeat the approach that we saw above for Batch Normalization, but this time for Dropout. Let us thus first have a look at the implementation and then compare two networks with each other where one is using Dropout and one is not. \n",
    "\n",
    "[1] Srivastava et al, \"Dropout: A Simple Way to Prevent Neural Networks from Overfitting\", 2014"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<!-- In case the image does not show, uncomment the following:<img name=\"dropout\" src=\"./img/dropout.jpg\"> -->\n",
    "<img name=\"dropout\" src=\"https://i2dl.vc.in.tum.de/static/images/exercise_08/dropout.jpg\">\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2 Dropout Implementation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dropout: Forward Pass\n",
    "\n",
    "The dropout method is a little less complex to implement than the Batch Normalization, hence we ask you to implement both, the forward and the backward pass. Let us start with the forward pass:\n",
    "\n",
    "<div class=\"alert alert-info\">\n",
    "    <h3>Task: Implement</h3>\n",
    "    <p> In the file <code>exercise_code/layers.py</code>, implement the forward pass for Dropout in <code>dropout_forward()</code>. Since Dropout behaves differently during training and testing, make sure to implement the operation for both modes.\n",
    "    </p>\n",
    "    <p> Once you have done so, run the cell below to test your implementation.\n",
    "    </p>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.random.randn(500, 500) + 10\n",
    "# Let us use different dropout values(p) for our dropout layer and see their effects\n",
    "for p in [0.3, 0.6, 0.75]:\n",
    "    out, _ = dropout_forward(x, {'mode': 'train', 'p': p})\n",
    "    out_test, _ = dropout_forward(x, {'mode': 'test', 'p': p})\n",
    "\n",
    "    print('Running tests with p = ', p)\n",
    "    print('Mean of input: ', x.mean())\n",
    "    print('Mean of train-time output: ', out.mean())\n",
    "    print('Mean of test-time output: ', out_test.mean())\n",
    "    print('Fraction of train-time output set to zero: ', (out == 0).mean())\n",
    "    print('Fraction of test-time output set to zero: ', (out_test == 0).mean())\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dropout: Backward Pass\n",
    "\n",
    "<div class=\"alert alert-info\">\n",
    "    <h3>Task: Implement</h3>\n",
    "    <p> In the file <code>exercise_code/layers.py</code>, implement the backward pass for dropout in <code>dropout_backward()</code>. After doing so, run the following cell to numerically gradient-check your implementation.\n",
    "    </p>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.random.randn(10, 10) + 10\n",
    "dout = np.random.randn(*x.shape)\n",
    "\n",
    "dropout_param = {'mode': 'train', 'p': 0.8, 'seed': 123}\n",
    "out, cache = dropout_forward(x, dropout_param)\n",
    "dx = dropout_backward(dout, cache)\n",
    "dx_num = eval_numerical_gradient_array(lambda xx: dropout_forward(xx, dropout_param)[0], x, dout)\n",
    "\n",
    "print('dx relative error: ', rel_error(dx, dx_num))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.3 Using Dropout with PyTorch\n",
    "\n",
    "Same experiment as for Batch Normalization: We will train a pair of two-layer networks on a training dataset where one network will use no Dropout and one will use a Dropout probability of 0.75. We will then visualize the training and validation accuracies of the two networks over time."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setup TensorBoard\n",
    "\n",
    "In exercise 07 you've already learned how to use TensorBoard. Let's use it again to make the debugging of our network and training process more convenient! Throughout this notebook, feel free to add further logs or visualizations to your TensorBoard!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Few Hyperparameters before we start things off\n",
    "hidden_dim = 200\n",
    "batch_size = 50\n",
    "\n",
    "epochs = 5\n",
    "learning_rate = 0.00005\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "################# COLAB ONLY #################\n",
    "# %load_ext tensorboard\n",
    "# %tensorboard --logdir=./ --port 6006\n",
    "\n",
    "# Use the cmd for less trouble if you can."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-success\">\n",
    "    <h3>Task: Check Code</h3>\n",
    "    <p> As before, we have already implemented those two networks for you. You may check them out in <code>exercise_code/BatchNormModel.py</code>. As always, feel free to play around with the parameters here. Run the following two cells to setup both models and train them for a few epochs in order to compare the performance with and without Dropout. \n",
    " </p>\n",
    "</div>\n",
    "\n",
    "### Train a model without Dropout\n",
    "\n",
    "Let us first start with a simple network `SimpleNetwork` which does not make use of Dropout."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train a model without Dropout\n",
    "model = SimpleNetwork(hidden_dim=hidden_dim, batch_size=batch_size, learning_rate=learning_rate)\n",
    "path = os.path.join('logs', 'dropout_model_logs')\n",
    "if os.path.exists(path):\n",
    "    shutil.rmtree(path)\n",
    "path = os.path.join(path, f'simple-model')\n",
    "tb_logger = SummaryWriter(path)\n",
    "\n",
    "# Train the classifier.\n",
    "train_dl, val_dl, _ = model.prepare_data()\n",
    "\n",
    "epochs = 5\n",
    "loss_func = F.cross_entropy # The loss function we use for regression (Could also be nn.L1Loss()).\n",
    "train_model(model, train_dl, val_dl, loss_func, tb_logger, epochs=epochs, name='Dropout')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train a model with Dropout\n",
    "\n",
    "Now that we have already seen how our simple network should work, let us look at the model `DropoutNetwork` that is actually using Dropout."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train a model with Dropout\n",
    "model = DropoutNetwork(hidden_dim=hidden_dim, batch_size=batch_size, learning_rate=learning_rate,dropout_p=0.2)\n",
    "path = os.path.join('logs', 'dropout_model_logs')\n",
    "path = os.path.join(path, f'dropout-model')\n",
    "tb_logger = SummaryWriter(path)\n",
    "\n",
    "# Train the classifier.\n",
    "train_dl, val_dl, _ = model.prepare_data()\n",
    "\n",
    "epochs = 5\n",
    "loss_func = F.cross_entropy # The loss function we use for regression (Could also be nn.L1Loss()).\n",
    "train_model(model, train_dl, val_dl, loss_func, tb_logger, epochs=epochs, name='Dropout')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Observations\n",
    "\n",
    "Take a look at TensorBoard to compare the performance of both networks:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext tensorboard\n",
    "%tensorboard --logdir dropout_logs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By using Dropout, it becomes evident that the Training Loss may increase; however, the model tends to exhibit improved performance on the Validation Set. Similar to Batch Normalization, Dropout demonstrates distinct behavior during training and testing phases."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
